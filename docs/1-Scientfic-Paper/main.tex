\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{mystyle}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automatic Parallelisation of Rust Programs at Compile Time}

\author{\IEEEauthorblockN{Michael Oultram}
\IEEEauthorblockA{Student ID: 1428105}
\and
\IEEEauthorblockN{Dr Ian Batten}
\IEEEauthorblockA{Project Supervisor}}

\maketitle

\begin{abstract}
  Processors have been gaining more multi-core performance which sequential code cannot take advantage of. One solution to this problem is to automatically convert sequential source code into parallelised source code. The literature for this topic is explored and it is split into two main areas: theoretical models of automatic parallelisation and real-world parallelising compilers. Three sequential elements of programs are converted manually by this paper and a design is outlined to automate these conversions in a new parallelising compiler for the safe programming language rust. 
\end{abstract}

\section{Introduction}
\textcite{Kish2002} estimated the end of Moore's Law of miniaturization within 6-8 years or earlier (based on their publication date) and as such, manufacturers have been increasing processors' core count to increase processor performance \parencite{Geer2005}. Writing parallelised programs to take advantage of these additional cores has some difficulty and often requires significant changes to the source code.

\section{Literature Review}
\subsection{Parallelisation Models}
\subsubsection{Static parallelism}
% Polyhedral model and affine transformation
\textcite{Feautrier1992} describes one model of a parallel program as a set of operations $\Omega$ on an initial store, and a partial ordering binary relation $\Gamma$ also known as a dependency tree. \textcite{Feautrier1992} also shows that this basic model of a parallel is equivalent to affine scheduling, where $\Omega$ and $\Gamma$ are described as linear inequalities.

\textcite{Bondhugula2008} describes the affine transformations required to allow for communication-minimized parallelisation and locality optimization. The Polyhedral Model is far too confusing...

% Iteration space slicing > affine transformations
\textcite{Beletska2011} shows that iteration space slicing extracts more coarse-grained parallelism than Affine Transformation Framework

\subsubsection{Speculative parallelism}
% Third option: Speculation
\textcite{Prabhu2010} provides an API for C\# to allow the programmer to specify areas of speculative parallelism. \textcite{Yiapanis2015} moves this into the compiler.

\subsection{Parallelisation Implementations}
Most research is focused on FORTRAN and the DO loops \parencite{Banerjee1993}.

OpenMP \parencite{Dagum1998,Lam2011}.

Some people have converted C-to-CUDA \parencite{Baskaran2010, Verdoolaege2013}.

\section{Problem Details}
\todo{Write about what exactly I want to parallelise. Which methods from the literature am I following and why? Include what rust is, what plugins can do in this section}

The rust compiler allows for plugins of different types. A syntax extension plugin can modify the abstract syntax tree of any annotated function. An early lint pass plugin can see any of the functions, with macros expanded, but it cannot edit them. The rust compiler executes syntax extension plugins first, and then the linter plugins.

I will focus mostly on attempting static parallelism. The literature focuses on unsafe languages such as C/C++ and FORTRAN and as a result, most of their methods revolve around restructuring the program into a more readable state. I will not focus on this as I am using a safe language (and it looked real hard).
Below are some examples of sequential code optimisations that I would like to automate using a rust compiler plugin. They are ordered based on their complexity.

\todo{For optimisations: Describe what is slow and what is required for optimisation. Good example sequential and parallel. Explain why/when faster (i.e. long list). Bad example of sequential. Explain why it cannot be converted}

\subsection{Parallel Function Optimisations}
If the function's arguments are not modifiable references and the function does not contain an unsafe block, then the function can be run in parallel.

\subsubsection{Fibonacci}

\autoref{code:seq-fibonacci} is a program that calculates Fibonacci numbers, written in a very inefficient way. The main method knows that $i = 10$ on the first line, and since it is not mutable, this cannot be changed. But the main method does `a lot of stuff' which will not affect the \texttt{fibonacci} function before calling it. Just running the \texttt{fibonacci} function earlier would not improve performance, as now `a lot of stuff' must wait for \texttt{fibonacci} to finish. Ideally, we want to start calculating \texttt{fibonacci} at the very beginning at the same time as doing `a lot of stuff' and wait for the result when we need it.

\begin{algorithm}
\caption{Sequential Fibonacci Function}
\label{code:seq-fibonacci}
\begin{minted}{rust}
fn main() {
    let i = 10;
    // A lot of stuff
    println!("{}", fibonacci(i));
}

fn fibonacci(n: u32) -> u64 {
    match n {
        0    => 0,
        1, 2 => 1,
        _    => fibonacci(n-1) + fibonacci(n-2),
    }
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Fibonacci Function}
\label{code:par-fibonacci}
\begin{minted}{rust}
fn main() {
    let i = 10;
    let fib = fibonacci_parallel(i);
    // A lot of stuff
    println!("{}", fib.join());
}

fn fibonacci_parallel(n: u32) -> JoinHandle<u64> {
    thread::spawn(move || {
        match n {
            0    => 0,
            1, 2 => 1,
            _    => {
                let n1 = fibonacci_parallel(n-1);
                let n2 = fibonacci_parallel(n-2);
                n1.join() + n2.join();
            },
        }
    })
}

fn fibonacci(n: u32) -> u64 {
    let fib = fibonacci_parallel(n);
    fib.join()
}
\end{minted}
\end{algorithm}

\autoref{code:par-fibonacci} shows one way of converting \autoref{code:seq-fibonacci} into a more parallelised version. These changes allow for fibonacci to start calculating as soon as i is decided, instead of waiting for `lots of stuff' to be executed. Also n1 and n2 are executed in parallel. fibonacci is changed to use the parallel version and then immediatley tries to get the result. This should allow for any external functions that are not modified to still work.

\subsection{For-Loop Optimisations}
If all the loop iterations are independent of each other, then we can run all the iterations at the same time.

\subsubsection{List manipulation}

\begin{algorithm}
\caption{Sequential For Loop}
\label{code:seq-for}
\begin{minted}{rust}
let mut list = vec![1,2,3,4,5];
for i in 0..list.len() {
    list[i] *= 2;
}
\end{minted}
\end{algorithm}

could be converted into
\todo{Make code below compile}

\begin{algorithm}
\caption{Parallel For Loop}
\label{code:par-for}
\begin{minted}{rust}
let mut list = vec![1,2,3,4,5];
// Create a communication channel
let (tx, rx) = mpsc::channel();
let handles = vec![];
// Start all threads
for i in 0..list.len() {
    let element = list[i];
    let tx = tx.clone();
    handles.push(thread::spawn(move || {
        let result = element * 2;
        tx.send((i, result));
    }));
}
// Wait for all threads to be finished
for handle in handles {
    handle.join();
}
// Receive all the results and update list
for _ in 0..list.len() {
    let (i, result) = rx.receive();
    list[i] = result;
}
\end{minted}
\end{algorithm}

In this simple case, the inner for loop has one operations and so the threaded version would be slower due to overhead. If the operation inside the loop was more complex, and the list was longer, then this conversion would make sense.

\subsubsection{Password cracker}
Real world example:
\begin{algorithm}
\caption{Sequential Password Cracker}
\label{code:seq-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>, hash_password: String) -> Option<String>{
    for word in dictionary {
        // Hash word using Sha256
        let mut hasher = Sha256::new();
        hasher.input_str(word);
        let hash_word = hasher.result_str();

        // Check if hash matches
        if hash_password == hash_word {
            return Some(word.clone());
        }
    }
    None
}
\end{minted}
\end{algorithm}

\todo{\autoref{code:par-password} finish}
\begin{algorithm}
\caption{Parallel Password Cracker}
\label{code:par-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>, hash_password: String) -> Option<String>{
    // Create a communication channel
    let (tx, rx) = mpsc::channel();
    let handles = vec![];
    let hashes = dictionary.clone();
    for i in 0..dictionary.len() {
        let word = dictionary[i];
        let tx = tx.clone();
        handles.push(thread::spawn(move || {
            // Hash word using Sha256
            let mut hasher = Sha256::new();
            hasher.input_str(word);
            hashes[i] = hasher.result_str();
            // Check if hash matches
            if hash_password == hash_word {
                tx.send((i, Some(word.clone())));
            } else {
                tx.send((i, None));
            }
        }));
    }

    // Receive all the results
    // Have to return same result as sequential
    let mut results = vec![];
    for _ in 0..list.len() {
        let (i, result) = rx.receive();
        //TODO: FINISH THIS
    }

    None
}
\end{minted}
\end{algorithm}

\subsection{Branch Optimisations}
In the previous optimisations, all the code that is run in parallel would have be run in sequential normally. This optimisation is for if statements which have a very slow condition. Each side of the branch is run in parallel, and then when the condition is finally worked out, the correct branch is kept.

\section{Design}
\todo{Describe how I will use the features of rust plugins}
The analysis stage is run by the linter and the modification stage is run by the syntax extension plugin. Analysis stage must come before the modification stage, so compiling is done twice (once for each stage).

When the plugin is loaded, it determines which stage it is by looking for a .auto-parallelize file. If this file does not exist, then it is the analysis stage. If the file does exist, the files content is loaded into a struct using the `serde\_json' crate and the stage is updated to be the modification stage.

\subsection{Analysis stage}
On the first compilation, the syntax extension plugin would do nothing. The linter plugin would view the entire abstract data tree and analyse what each statement depends on and modifies. This would create a dependency tree, where any two statements that are independent can be run in parallel. The linter plugin would use this dependency tree to determine which parts should be parallelised, and save this information to a file.

Detecting the end of the analysis stage required some work.

\subsection{Modification stage}
On the second compilation, the syntax extension would be able to read the file the linter plugin created on the previous compilation. This lists all the changes required and the syntax plugin can apply those changes to the abstract syntax tree function by function. The linter plugin would also be able to view this file, and it could produce compiler warnings for any function that could be parallelised that is missing an annotation.

\printbibliography
\end{document}
