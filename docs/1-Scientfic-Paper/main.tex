\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{mystyle}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automatic Parallelisation of Rust Programs at Compile Time}

\author{\IEEEauthorblockN{Michael Oultram}
\IEEEauthorblockA{Student ID: 1428105}
\and
\IEEEauthorblockN{Dr Ian Batten}
\IEEEauthorblockA{Project Supervisor}}

\maketitle

\begin{abstract}
  Processors have been gaining more multi-core performance which sequential code cannot take advantage of. Many solutions exist to this problem including automatic parallelisation at the binary level, automatic parallelisation at the compiler and manually parallelising using annotations. The potential benefits of solving this problem are increased performance for existing programs, as well as making development of new software easier (as programmers do not need to worry about writing parallelised code).

  This paper focuses on automatically converting sequential source code into a parallelised program. The literature is explored and presenting into two main areas: theoretical models of automatic parallelisation and existing real-world parallelising compilers.
  The author presents a design for a new parallelising compiler for the rust programming language. Three sequential elements of Rust programs are manually converted by this paper.
\end{abstract}

\section{Introduction}
\textcite{Kish2002} estimated the end of Moore's Law of miniaturization within 6-8 years or earlier (based on their publication date) and as such, manufacturers have been increasing processors' core count to increase processor performance \parencite{Geer2005}. Writing parallelised programs to take advantage of these additional cores has some difficulty and often requires significant changes to the source code. One solution to this problem, which is the focus of this paper, is to automatically transform sequential source code into parallelised code. This solution, if achieved, would allow for existing (open-source) sequential programs to take advantage of the new hardware. It would also make developing new programs easier for programmers, as they can just write the easier sequential code and let the compiler make it run in parallel.

Section \ref{sec:related-work} examines existing implementations by other authors and finds that most solutions focus on unsafe languages like C++ and FORTRAN. Since a significant proportion of existing programs are written in these languages, it makes sense for other authors to focus on these languages. \todo{source?} There is a downside to focusing on these unsafe languages, dependency analysis becomes much more difficult. Because of this, the design aspects of this paper will use the safe programming language rust.

``Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety'' \parencite{rustlang}. This brief introduction to some of Rust's features will explain the elements of the language necessary for the reader to understand for later sections of this paper. For further understanding of the language, it is recommended that the reader looks at \textcite{rustbook}.

In C++ (or other unsafe languages), a variable can be accessed and modified if it is in scope. In rust, each variable is immutable by default and if a variable is passed to another function, then that function takes `ownership' of the variable.

\begin{minted}{rust}
fn main() {
    let mut i = vec![1,2,3];
    f(i);
    g(i);
}
\end{minted}

The code above would not compile in rust, but similar code would compile in C++. When \texttt{f} is called, it takes ownership of the variable \texttt{i}. This essentially moves it out of scope, and so it cannot be moved into \texttt{g}. Instead of moving the ownership of \texttt{i} to \texttt{f}, we could let \texttt{f} `borrow' the variable.

\begin{minted}{rust}
fn main() {
    let mut i = vec![1,2,3];
    f(&mut i);
    g(&i);
}
\end{minted}

In this example which does compile, variable \texttt{i} is `mutably borrowed' by \texttt{f}, and then `immutably borrowed' by \texttt{g}. The syntax of rust makes it explicit that \texttt{f} may modify this variable and \texttt{g} cannot. This information expressed is available to the compiler. If we did not want to modify the types of \texttt{f} and \texttt{g}, we could clone \texttt{i} instead.

\begin{minted}{rust}
fn main() {
    let mut i = vec![1,2,3];
    let j = i.clone();
    f(i);
    g(j);
}
\end{minted}

In this example, we make a copy of \texttt{i} before moving it to \texttt{f}. Any changes that \texttt{f} makes to \texttt{i} will not be transferred to \texttt{g}, unlike the previous example. Note: \texttt{i} must be cloned before moving \texttt{i} to \texttt{f}, or else we would not have access to \texttt{i} to clone it.

%\newpage
\section{Related Work}
\label{sec:related-work}

\subsection{Parallelisation Models}
\label{sec:related-models}
In this section, we look at theoretical models of automatic parallelism. The static parallelism subsection shows related work where the schedule is fixed and calculated at `compile' time. It is shown how rearranging loop iterations and optimise the memory access patterns and increase parallelising performance. The speculative parallelism subsection shows related work where the schedule is more flexible. This kind of parallelism tries to run dependent tasks in parallel and detecting when there is a conflict. When a conflict occurs, some parallel thread is `undone' and rerun.

\subsubsection{Static parallelism}
% Polyhedral model and affine transformation
\textcite{Feautrier1992,Feautrier1992a} describes one model of a parallel program as a set of operations $\Omega$ on an initial store, and a partial ordering binary relation $\Gamma$ also known as a dependency tree. It is shown that this basic model of a parallel is equivalent to affine scheduling, where $\Omega$ and $\Gamma$ are described as linear inequalities. Finding a solution where these linear inequalities hold produces a schedule for the program where dependent statements are executed in order. There are some programs where no affine schedule exists.
\textcite{Bondhugula2008} uses the affine scheduling model on perfectly, and imperfectly nested loops. They describe the transformations needed to minimise the communication between threads, further increasing the performance of the parallelise.

% Iteration space slicing > affine transformations
A different method to affine scheduling is iteration space slicing introduced by \textcite{Pugh1997}. ``Iteration space slicing takes dependency information as input to find all statement instances from a given loop nest which must be executed to produce the correct result''. \textcite{Pugh1997} shows how this information can be used to transform loops on example programs to produce a real world speedup. \textcite{Beletska2011} shows that iteration space slicing extracts more coarse-grained parallelism than affine scheduling.

\subsubsection{Speculative parallelism}
% Third option: Speculation
\textcite{Zhong2008} shows that some parallelisable optimisations are hidden in loops, such that affine scheduling and iteration space splicing cannot find. They propose a method that runs future loop iterations in parallel with past loop iterations. If a future loop iteration accesses some shared memory space, and then a past iteration modifies that location, the future loop iteration is `undone' and restarted. It is shown that this method increases the amount of the program that is parallelised.

\textcite{Prabhu2010} introduce two new language constructs for C\# to allow the programmer to manually specify areas of the program that can be speculatively parallelised. \textcite{Yiapanis2015} designs a parallelising compiler which can take advantage of speculative parallelism.

\subsection{Parallelisation Implementations}
In this section we look at some of the implemented parallelising compilers based off of the models described in Section \ref{sec:related-models}. This includes parallelising compilers which focus on parallelising FORTRAN programs; OpenMP which is an model for shared memory programming and parallelising compilers which convert sequential CPU code into parallelised GPU code.

\textcite{Eigenmann1998} manually parallelises the PERFECT benchmarks for FORTRAN. which are compared with the original versions to calculate the potential speedup of an automatic parallelising compiler.
\textcite{DHollander1998} developed a FORTRAN transformer which reconstructs code using GOTO statements so that more parallelisms can be detected, performs dependency analysis and automatically parallelised loops. Jobs can be split between networked machines to increase performance further.
\textcite{Rauchwerger1999} introduce a new language construct for FORTRAN programs which allows for run-time speculative parallelism on for loops. \todo{PERFECT benchmarks}

\textcite{Quinones2005} introduce the Mitosis compiler which combines speculation with iteration space slicing. \todo{Finish}

\textcite{Dagum1998} introduces a programming interface for shared memory multiprocessors called OpenMP targeted at FORTRAN, C and C++. The programmer annotates the elements of the program that are parallelisable, which the compiler recognises and performs the optimisation. \textcite{Dagum1998} compares OpenMP to alterative parallel programming models.
\textcite{Kim2000} introduces the ICP-PFC compiler for FORTRAN. All loops in the source code are analysed by calculated a dependency matrix. The compiler automatically adds the relevant OpenMP annotations to the loop.
\textcite{Lam2011} extends OpenMP using machine learning to automate the parallelisation. The system is trained using a set containing programs already parallelised using OpenMP. The knowledge learned is applied to sequential programs to produce parallelised programs.

A CPUs architecture is typically optimised for latency whereas a GPUs architecture is typically optimised for throughput. This can make GPUs perform much better that CPUs for a certain type of task. \textcite{Baskaran2010} uses the affine transformation model to convert sequential C code into parallelised CUDA code. For loops are tiled such that they can be executed efficiently on the GPU.

\newpage
\section{Problem Details}
\label{sec:problem-details}
The literature focuses on unsafe languages such as C/C++ and FORTRAN. As a result, most of their methods revolve around understanding conflicts between statements/loop iterations. Due to the different memory model in Rust, the dependency information that the literature's approaches tried to gather is more readily available.

The Rust compiler allows for plugins of different types but there are two types that are of interest to this problem. Written in the order of execution, they are:
\begin{itemize}
    \item Syntax Extension: can modify the abstract syntax tree of any annotated function.
    \item Early Lint Pass: can see abstract syntax tree of each uncompiled function, with macros expanded, without annotations, but it cannot edit them.
\end{itemize}

To automate the parallelisation of Rust programs, the process must be manually to understand the problem. This section looks at some examples of sequential rust code and their manually calculated optimisations. To simplify the explanation of how a sequential program could be parallelised, threads are used as if they have no overhead. In reality this is not the case and so in \autoref{sec:design}, where this process is automated using a Rust compiler plugin, \texttt{thread::spawn} is replaced with something more optimal.

\subsection{Parallel Function Optimisations}
In some cases the arguments of a function are known well before the result of the function is required. In sequential programs, only one function is executed at once and so when the result is required, the program switches to that function. This function could be started in the background as soon as the arguments are decided if the program was parallelised. This would allow the result to be ready for when it is requested or at least closer to ready than the pure sequential code.
In Rust terms, if the function's arguments are not modifiable references and the function does not contain an unsafe block, then the function can be run in parallel.
\todo{Check this fact}

\textit{Example:}
%\subsubsection{Fibonacci}
\autoref{code:seq-fibonacci} is a program that calculates Fibonacci numbers, written in a very inefficient way. The main method knows that $i = 10$ on the first line, and since it is not mutable, this cannot be changed. But the main method does `a lot of stuff' which will not affect the \texttt{fibonacci} function before calling it. Just running the \texttt{fibonacci} function earlier would not improve performance, as now `a lot of stuff' must wait for \texttt{fibonacci} to finish. Ideally, we want to start calculating \texttt{fibonacci} at the very beginning at the same time as doing `a lot of stuff' and wait for the result when we need it.

\begin{algorithm}
\caption{Sequential Fibonacci Function}
\label{code:seq-fibonacci}
\begin{minted}{rust}
fn main() {
    let i = 10;
    // A lot of stuff
    println!("{}", fibonacci(i));
}

fn fibonacci(n: u32) -> u64 {
    match n {
        0    => 0,
        1, 2 => 1,
        _    => fibonacci(n-1) + fibonacci(n-2),
    }
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Fibonacci Function}
\label{code:par-fibonacci}
\begin{minted}{rust}
fn main() {
    let i = 10;
    let fib = fibonacci_parallel(i);
    // A lot of stuff
    println!("{}", fib.join());
}

fn fibonacci_parallel(n: u32) -> JoinHandle<u64> {
    thread::spawn(move || {
        match n {
            0    => 0,
            1, 2 => 1,
            _    => {
                let n1 = fibonacci_parallel(n-1);
                let n2 = fibonacci_parallel(n-2);
                n1.join() + n2.join();
            },
        }
    })
}

fn fibonacci(n: u32) -> u64 {
    let fib = fibonacci_parallel(n);
    fib.join()
}
\end{minted}
\end{algorithm}

\autoref{code:par-fibonacci} shows one way of converting \autoref{code:seq-fibonacci} into a more parallelised version. These changes allow for fibonacci to start calculating as soon as i is decided, instead of waiting for `lots of stuff' to be executed. Also n1 and n2 are executed in parallel. fibonacci is changed to use the parallel version and then immediatley tries to get the result. This should allow for any external functions that are not modified to still work.

\subsection{For-Loop Optimisations}
If all the loop iterations are independent of each other, then we can run all the iterations at the same time. However, in most cases, loops are only partially parallelisable.

\textit{Example:}
%\subsubsection{Password cracker}
\autoref{code:seq-password} is a real world example where the for loop is combined with an if statement which returns the password for the first valid hash. If we were to naively put the contents of the for loop into separate threads, as shown in \autoref{code:par-naive-password}, then we may end up with the wrong result. In the sequential for loop, the first password in the list to match the hash would be returned. In the naive parallel version, the password returned depends on the order the threads are run.

\begin{algorithm}
\caption{Sequential Password Cracker}
\label{code:seq-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    for word in dictionary {
        // Hash word using Sha256
        let mut sha = Sha256::new();
        sha.input_str(word);
        let hash_word = sha.result_str();
        // Check if hash matches
        if hash_password == hash_word {
            return Some(word.clone());
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Naive Parallel Password Cracker}
\label{code:par-naive-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    // Create a communication channel
    let (tx, rx) = mpsc::channel();
    // Start a thread for each dictionary entry
    for i in 0..dictionary.len() {
        let word = dictionary[i].clone();
        let tx = tx.clone();
        thread::spawn(move || {
            // Hash word using Sha256
            let result = {
                let mut sha = Sha256::new();
                sha.input_str(word);
                let hash_word = sha.result_str();
                // Send result via channel
                if hash_password == hash_word {
                    Some(word)
                } else {
                    None
                }
            };
            tx.send(result);
        });
    }
    // Receive up to dictionary.len() results
    for _ in 0..dictionary.len() {
        if let Some(result) = rx.receive() {
            return result;
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\autoref{code:seq-refactor-password} shows that we split the for loop into two parts, the hashing part (which is parallelisable) and the verifying part (which is not as parallelisable if we want to keep the order). \autoref{code:par-password} is the final complete parallelisation of \autoref{code:seq-password}. Each word is hashed in it's own thread and the hash is compared to \texttt{hash\_password} and it is returned and stored in the initial thread. The function does not return until a result until all previous threads have also returned.

\begin{algorithm}
\caption{Refactored Sequential Password Cracker}
\label{code:seq-refactor-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    let mut hashes = vec![];
    for word in dictionary {
        // Hash word using Sha256
        let mut sha = Sha256::new();
        sha.input_str(word);
        let hash_word = sha.result_str();
        hashes.push(hash_word);
    }
    // Check if hash matches
    for hash_word in hashes {
        if hash_password == hash_word {
            return Some(word.clone());
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Password Cracker}
\label{code:par-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    // Create a communication channel
    let (tx, rx) = mpsc::channel();
    for i in 0..dictionary.len() {
        let word = dictionary[i].clone();
        let tx = tx.clone();
        thread::spawn(move || {
            // Hash word using Sha256
            let result = {
                let mut sha = Sha256::new();
                sha.input_str(word);
                let hash_word = sha.result_str();
                // Check if hash matches
                if hash_password == hash_word {
                    Some(word)
                } else {
                    None
                }
            };
            tx.send((i, result));
        });
    }
    // Receive all the results
    // Have to return same result as sequential
    let mut results = vec![None; list.len()];
    let mut verified_upto = -1;
    for _ in 0..results.len() {
        // Receive result and store in location
        let (i, result) = rx.receive();
        results[i] = Some(result);
        // Check for final result
        for i in 0..results.len() {
            if let Some(result) = results[i] {
                if let Some(word) = result {
                    return word;
                }
            } else {
                // Have not received i result yet
                break;
            }
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\subsection{Branch Optimisations}
In the previous optimisations, all the code that is run in parallel would have be run in sequential normally. This optimisation is for \texttt{if} statements which have a very slow condition, as shown by \autoref{code:seq-slowif}. Each side of the branch could be run in parallel whilst waiting for the condition. When the condition is finally calculated, the result of the correct branch is kept. \autoref{code:par-slowif} shows this optimisation applied to \autoref{code:seq-slowif}.

\begin{algorithm}
\caption{Sequential Slow If}
\label{code:seq-slowif}
\begin{minted}{rust}
fn f(a: u32, b: u32) -> u32 {
    if slow_condition(a, b) {
        let c = b - a;
        (a * c) + 5
    } else {
        let c = a * b;
        (c + 19) ^ 2
    }
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Slow If}
\label{code:par-slowif}
\begin{minted}{rust}
fn f(a: u32, b: u32) -> u32 {
    let true_branch = {
        let a = a.clone();
        let b = b.clone();
        thread::spawn(move || {
            let c = b - a;
            (a * c) + 5
        })
    };
    let false_branch = {
        let a = a.clone();
        let b = b.clone();
        thread::spawn(move || {
            let c = a * b;
            (c + 19) ^ 2
        })
    };
    if slow_condition(a, b) {
        true_branch.join()
    } else {
        false_branch.join()
    }
}

\end{minted}
\end{algorithm}

This concept can be expanded for when there are multiple branches such as \texttt{match} statements. Ideally, if the incorrect branch is still being calculated in a thread, then this thread should be cancelled.

\section{Design Overview}
\label{sec:design}
A typical parallelising compiler, such as those described in \autoref{sec:related-work}, have a few stages. First the compiler looks at each statement of the source code, and perform dependency analysis to calculate the critical path. Any independent statements can be run in parallel. A scheduling algorithm calculates which order the statements are executed, grouping statements that are dependent on each other into the same task. A compile time performance metric is used on each task to estimate the potential speedup of the parallel version over the sequential version. Due to the overhead of threads, this potential speedup would not actually be achieved. The parallelising compiler takes this into account and will only use the parallel version if it predicts it will really be faster.

This papers design takes elements from a typical parallelising compiler, dividing the task into two main stages, an analysis stage and a modification stage. The analysis stage is run by the linter and the modification stage is run by the syntax extension plugin. Analysis stage must come before the modification stage, so compiling must be done twice (once for each stage).
When the plugin is loaded, it determines which stage it is by looking for a shared JSON file. If this file does not exist, then it is the analysis stage. If the file does exist, the files content is loaded and the stage is updated to be the modification stage.

\subsection{Analysis stage}
On the first compilation, the syntax extension plugin would do nothing. The linter plugin would view the entire abstract data tree and analyse what each statement depends on and modifies. This would create a dependency tree, where any two statements that are independent can be run in parallel. The linter plugin would use this dependency tree to determine which parts should be parallelised, and save this information to a file.

\todo{Expand}

%Detecting the end of the analysis stage required some work.

\subsection{Modification stage}
On the second compilation, the syntax extension would be able to read the file the linter plugin created on the previous compilation. This lists all the changes required and the syntax plugin can apply those changes to the abstract syntax tree function by function. The linter plugin, when run for the second time, would also be able to view this file, and it could produce compiler warnings for any function that could be parallelised that is missing an annotation.

The modifications applied would be similar to the sample programs shown in \autoref{sec:problem-details}. This sample programs assume that threads have no overhead and were used as a method of describing how a sequential program could be run in parallel. In the real world, these parallelised sample programs produce an unreasonable amount of threads of which most are waiting. This can cause the parallel version to perform a lot worse than sequential code. The following subsections describe some of the problems with the parallelised sample programs, and provides some solutions.

\subsubsection{Infinite Thread Problem}
The initial solution for too many threads is to use a thread-pool so only a fixed number of threads are executed at the same time. This solution would not work in this case as if all the threads are waiting on a future task, which is also waiting for a thread to be free then we get stuck in a deadlock. By tweaking the design of the thread-pool slightly, we can have a fixed the number of tasks and prevent this deadlock.

As the task queue is the main cause of this deadlock, it is removed from this new proposed design which will be referred to as a no-queue thread-pool. Whenever a task is created, it looks for an available thread and that thread starts executing the task in the background. If there is no thread available, then the current thread should execute the task. Also, if the the current thread is waiting for a task to return, then it could execute another task whilst waiting. \todo{Display this if statement more visually?} \todo{Explain why this design won't deadlock like the traditional thread-pool}

To modify the sample programs to use the no-queue thread-pool would be of minimal work; it would require a shared memory space (to gain access to the threads) and instead of calling \texttt{thread::spawn}, it would call another function.

\subsubsection{Performance Analysis}
Parallelising small tasks, or tasks that would require a lot of synchronisation when run in parallel actually run significantly slower than the sequential code. In these cases, the code should not be parallelised. There is a problem though, how does the compiler know if it is faster parallelised or sequential.

One possible solution is to run the sequential version in one thread, and the parallel version in the other threads. Which-ever version finishes first is used, and the other version is cancelled. This approach leaves the performance approach until run-time, but it is a very inefficient solution as the parallel version now has fewer threads.

Another solution is to use some sort of performance metric at compile time to estimate which version will be faster. If we assume each statement takes one unit of execution time, we can attempt to calculate how many time units the sequential and parallel version will take. Since we know that there is an overhead to the parallel version, we should add on some extra time units. The faster version would be the version that is compiled. This solution would not really work as described as the execution time may be dependent on how many iterations of a loop (which isn't necessarily known at compile time). The parallel version is also dependent on the number of cores the machine has, also not known at compile time.

The best solution is a combination of the previous two ideas. The compiler should create an function which estimates both the sequential and the parallel version execution time. This function can take in any run-time conditions i.e. the number of iterations of a loop or the number of cores the machine has. This function should be fast such that the overhead of calculating which version is faster should be negligible. At run-time, this function predicts the faster version using the run-time variables, and the faster version is the only version run.

% TODO: Equalising
%\newpage
%\enlargethispage{-55mm}
\section{References}
\printbibliography[heading=none]
\end{document}
