\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{mystyle}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automatic Parallelisation of Rust Programs at Compile Time}

\author{\IEEEauthorblockN{Michael Oultram}
\IEEEauthorblockA{Student ID: 1428105}
\and
\IEEEauthorblockN{Dr Ian Batten}
\IEEEauthorblockA{Project Supervisor}}

\maketitle

\begin{abstract}
  Processors have been gaining more multi-core performance which sequential code cannot take advantage of. One solution to this problem is to automatically convert sequential source code into parallelised source code. The literature for this topic is explored and it is split into two main areas: theoretical models of automatic parallelisation and real-world parallelising compilers. Three sequential elements of Rust programs are converted manually by this paper and a design is outlined to automate these conversions in a new parallelising compiler for the safe programming language Rust.
\end{abstract}

\section{Introduction}
\textcite{Kish2002} estimated the end of Moore's Law of miniaturization within 6-8 years or earlier (based on their publication date) and as such, manufacturers have been increasing processors' core count to increase processor performance \parencite{Geer2005}. Writing parallelised programs to take advantage of these additional cores has some difficulty and often requires significant changes to the source code.

\todo{Paragraphs do not link yet}

``Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety'' \parencite{rustlang}. This brief introduction to Rust will explain the elements of the language necessary for the reader to understand for later sections of this paper. For further understanding of the language, it is recommended that the reader looks at \textcite{rustbook}.

\todo{Explain the Rust language syntax and memory model}

\section{Related Work}
\label{sec:related-work}
\todo{Write up and cleanup. Also read the papers}
\todo{Include \textcite{Zhong2008} somewhere}
\subsection{Parallelisation Models}
\subsubsection{Static parallelism}
% Polyhedral model and affine transformation
\textcite{Feautrier1992,Feautrier1992b} describes one model of a parallel program as a set of operations $\Omega$ on an initial store, and a partial ordering binary relation $\Gamma$ also known as a dependency tree. It is shown that this basic model of a parallel is equivalent to affine scheduling, where $\Omega$ and $\Gamma$ are described as linear inequalities.
%\textcite{Bondhugula2008} uses the affine scheduling model on perfectly, and imperfectly nested loops. They describe the transformations needed on the source code to minimise the communication between threads, further increasing the performance of the parallelise

% Iteration space slicing > affine transformations
A different method to affine scheduling is iteration space slicing introduced by \textcite{Pugh1997}. \todo{Describe iteration space slicing}
\textcite{Beletska2011} shows that iteration space slicing extracts more coarse-grained parallelism than Affine Transformation Framework.

\subsubsection{Speculative parallelism}
% Third option: Speculation
\textcite{Prabhu2010} provides an API for C\# to allow the programmer to specify areas of speculative parallelism.
\textcite{Yiapanis2015} moves this into the compiler.

\textcite{Prabhu2010} does manual speculative parallelism in a safe way.

\subsection{Parallelisation Implementations}
\textcite{DHollander1998} converts FORTRAN and DO loops.
\textcite{Eigenmann1998} parallelises benchmarks using FORTRAN.

\textcite{Rauchwerger1999} uses FORTRAN speculatively on for loops.
\textcite{Quinones2005} uses speculation and iteration space slicing together.

OpenMP is a programming interface for shared memory programming.
\textcite{Dagum1998} compares OpenMP to alterative parallel programming models.
\textcite{Kim2000} converts FORTRAN using OpenMP automatically.
\textcite{Lam2011} extends OpenMP using machine learning to automate the parallelisation.

GPUs are another focus due to their number of threads.
\textcite{Baskaran2010} have converted C-to-CUDA.
\textcite{Verdoolaege2013} generates CUDA code using the Polyhedral model.

\section{Problem Details}
\label{sec:problem-details}
\todo{Write about what exactly I want to parallelise. Which methods from the literature am I following and why? Include what Rust is, what plugins can do in this section}

The literature focuses on unsafe languages such as C/C++ and FORTRAN and as a result, most of their methods revolve around understanding conflicts between iterations. Due to the different memory model in Rust, the dependency information that the literature's approaches tried to gather is readily available.

The Rust compiler allows for plugins of different types but there are two types that are of interest to this problem. Written in the order of execution, they are:
\begin{itemize}
    \item Syntax Extension: can modify the abstract syntax tree of any annotated function.
    \item Early Lint Pass: can see abstract syntax tree of each uncompiled function, with macros expanded, without annotations, but it cannot edit them.
\end{itemize}

To automate the parallelisation of Rust programs, the process must be manually to understand the problem. Below are some examples of sequential code and their manually calculated optimisations that could be automated using a Rust compiler plugin. They are ordered based on their complexity.

\todo{For optimisations: Describe what is slow and what is required for optimisation. Good example sequential and parallel. Explain why/when faster (i.e. long list). Bad example of sequential. Explain why it cannot be converted}

\subsection{Parallel Function Optimisations}
In some cases the arguments of a function are known well before the result of the function is required. In sequential programs, only one function is executed at once and so when the result is required, the program switches to that function. This function could be started in the background as soon as the arguments are decided if the program was parallelised. This would allow the result to be ready for when it is requested or at least closer to ready than the pure sequential code.
In Rust terms, if the function's arguments are not modifiable references and the function does not contain an unsafe block, then the function can be run in parallel.
\todo{Check this fact}

\textit{Example:}
%\subsubsection{Fibonacci}
\autoref{code:seq-fibonacci} is a program that calculates Fibonacci numbers, written in a very inefficient way. The main method knows that $i = 10$ on the first line, and since it is not mutable, this cannot be changed. But the main method does `a lot of stuff' which will not affect the \texttt{fibonacci} function before calling it. Just running the \texttt{fibonacci} function earlier would not improve performance, as now `a lot of stuff' must wait for \texttt{fibonacci} to finish. Ideally, we want to start calculating \texttt{fibonacci} at the very beginning at the same time as doing `a lot of stuff' and wait for the result when we need it.

\begin{algorithm}
\caption{Sequential Fibonacci Function}
\label{code:seq-fibonacci}
\begin{minted}{rust}
fn main() {
    let i = 10;
    // A lot of stuff
    println!("{}", fibonacci(i));
}

fn fibonacci(n: u32) -> u64 {
    match n {
        0    => 0,
        1, 2 => 1,
        _    => fibonacci(n-1) + fibonacci(n-2),
    }
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Fibonacci Function}
\label{code:par-fibonacci}
\begin{minted}{rust}
fn main() {
    let i = 10;
    let fib = fibonacci_parallel(i);
    // A lot of stuff
    println!("{}", fib.join());
}

fn fibonacci_parallel(n: u32) -> JoinHandle<u64> {
    thread::spawn(move || {
        match n {
            0    => 0,
            1, 2 => 1,
            _    => {
                let n1 = fibonacci_parallel(n-1);
                let n2 = fibonacci_parallel(n-2);
                n1.join() + n2.join();
            },
        }
    })
}

fn fibonacci(n: u32) -> u64 {
    let fib = fibonacci_parallel(n);
    fib.join()
}
\end{minted}
\end{algorithm}

\autoref{code:par-fibonacci} shows one way of converting \autoref{code:seq-fibonacci} into a more parallelised version. These changes allow for fibonacci to start calculating as soon as i is decided, instead of waiting for `lots of stuff' to be executed. Also n1 and n2 are executed in parallel. fibonacci is changed to use the parallel version and then immediatley tries to get the result. This should allow for any external functions that are not modified to still work.

\subsection{For-Loop Optimisations}
If all the loop iterations are independent of each other, then we can run all the iterations at the same time. However, in most cases, loops are only partially parallelisable.

\textit{Example:}
%\subsubsection{Password cracker}
\autoref{code:seq-password} is a real world example where the for loop is combined with an if statement which returns the password for the first valid hash. If we were to naively put the contents of the for loop into separate threads, as shown in \autoref{code:par-naive-password}, then we may end up with the wrong result. In the sequential for loop, the first password in the list to match the hash would be returned. In the naive parallel version, the password returned depends on the order the threads are run.

\begin{algorithm}
\caption{Sequential Password Cracker}
\label{code:seq-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    for word in dictionary {
        // Hash word using Sha256
        let mut sha = Sha256::new();
        sha.input_str(word);
        let hash_word = sha.result_str();
        // Check if hash matches
        if hash_password == hash_word {
            return Some(word.clone());
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Naive Parallel Password Cracker}
\label{code:par-naive-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    // Create a communication channel
    let (tx, rx) = mpsc::channel();
    // Start a thread for each dictionary entry
    for i in 0..dictionary.len() {
        let word = dictionary[i].clone();
        let tx = tx.clone();
        thread::spawn(move || {
            // Hash word using Sha256
            let result = {
                let mut sha = Sha256::new();
                sha.input_str(word);
                let hash_word = sha.result_str();
                // Send result via channel
                if hash_password == hash_word {
                    Some(word)
                } else {
                    None
                }
            };
            tx.send(result);
        });
    }
    // Receive up to dictionary.len() results
    for _ in 0..dictionary.len() {
        if let Some(result) = rx.receive() {
            return result;
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\autoref{code:seq-refactor-password} shows that we split the for loop into two parts, the hashing part (which is parallelisable) and the verifying part (which is not as parallelisable if we want to keep the order). \autoref{code:par-password} is the final complete parallelisation of \autoref{code:seq-password}. Each word is hashed in it's own thread and the hash is compared to \texttt{hash\_password}. The result of

\begin{algorithm}
\caption{Refactored Sequential Password Cracker}
\label{code:seq-refactor-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    let mut hashes = vec![];
    for word in dictionary {
        // Hash word using Sha256
        let mut sha = Sha256::new();
        sha.input_str(word);
        let hash_word = sha.result_str();
        hashes.push(hash_word);
    }
    // Check if hash matches
    for hash_word in hashes {
        if hash_password == hash_word {
            return Some(word.clone());
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Password Cracker}
\label{code:par-password}
\begin{minted}{rust}
fn crack_password(dictionary: &Vec<String>,
                  hash_password: String)
                  -> Option<String> {
    // Create a communication channel
    let (tx, rx) = mpsc::channel();
    for i in 0..dictionary.len() {
        let word = dictionary[i].clone();
        let tx = tx.clone();
        thread::spawn(move || {
            // Hash word using Sha256
            let result = {
                let mut sha = Sha256::new();
                sha.input_str(word);
                let hash_word = sha.result_str();
                // Check if hash matches
                if hash_password == hash_word {
                    Some(word)
                } else {
                    None
                }
            };
            tx.send((i, result));
        });
    }
    // Receive all the results
    // Have to return same result as sequential
    let mut results = vec![None; list.len()];
    let mut verified_upto = -1;
    for _ in 0..results.len() {
        // Receive result and store in location
        let (i, result) = rx.receive();
        results[i] = Some(result);
        // Check for final result
        for i in 0..results.len() {
            if let Some(result) = results[i] {
                if let Some(word) = result {
                    return word;
                }
            } else {
                // Have not received i result yet
                break;
            }
        }
    }
    // No hash matched
    None
}
\end{minted}
\end{algorithm}

\subsection{Branch Optimisations}
In the previous optimisations, all the code that is run in parallel would have be run in sequential normally. This optimisation is for \texttt{if} statements which have a very slow condition. Each side of the branch is run in parallel, and then when the condition is finally worked out, the correct branch is kept. This concept can be expanded for when there are multiple branches such as \texttt{match} statements.

\todo{Code Examples}

\begin{algorithm}
\caption{Sequential Slow If}
\label{code:seq-slowif}
\begin{minted}{rust}
fn slow_if(a: u32, b: u32) {
    if slow_condition(a, b) {
        let c = b - a;
        (a * c) + 5
    } else {
        let c = a * b;
        (c + 19) ^ 2
    }
}
\end{minted}
\end{algorithm}

\begin{algorithm}
\caption{Parallel Slow If}
\label{code:par-slowif}
\begin{minted}{rust}
fn slow_if(a: u32, b: u32) {
    let true_branch = {
        let a = a.clone();
        let b = b.clone();
        thread::spawn(move || {
            let c = b - a;
            (a * c) + 5
        })
    };
    let false_branch = {
        let a = a.clone();
        let b = b.clone();
        thread::spawn(move || {
            let c = a * b;
            (c + 19) ^ 2
        })
    };
    if slow_condition(a, b) {
        true_branch.join()
    } else {
        false_branch.join()
    }
}

\end{minted}
\end{algorithm}

\section{Design Overview}
A typical parallelising compiler, such as those described in \autoref{sec:related-work}, have a few stages. First the compiler looks at each statement of the source code, and perform dependency analysis to calculate the critical path. Any independent statements can be run in parallel. A scheduling algorithm calculates which order the statements are executed, grouping statements that are dependent on each other into the same task. A compile time performance metric is used on each task to estimate the potential speedup of the parallel version over the sequential version. Due to the overhead of threads, this potential speedup would not actually be achieved. The parallelising compiler takes this into account and will only use the parallel version if it predicts it will really be faster.

This papers design takes elements from a typical parallelising compiler, dividing the task into two main stages, an analysis stage and a modification stage.

\todo{Describe how I will use the features of Rust plugins}

The analysis stage is run by the linter and the modification stage is run by the syntax extension plugin. Analysis stage must come before the modification stage, so compiling is done twice (once for each stage).

When the plugin is loaded, it determines which stage it is by looking for a .auto-parallelize file. If this file does not exist, then it is the analysis stage. If the file does exist, the files content is loaded into a struct using the `serde\_json' crate and the stage is updated to be the modification stage.

\subsection{Analysis stage}
On the first compilation, the syntax extension plugin would do nothing. The linter plugin would view the entire abstract data tree and analyse what each statement depends on and modifies. This would create a dependency tree, where any two statements that are independent can be run in parallel. The linter plugin would use this dependency tree to determine which parts should be parallelised, and save this information to a file.

Detecting the end of the analysis stage required some work.

\subsection{Modification stage}
On the second compilation, the syntax extension would be able to read the file the linter plugin created on the previous compilation. This lists all the changes required and the syntax plugin can apply those changes to the abstract syntax tree function by function. The linter plugin would also be able to view this file, and it could produce compiler warnings for any function that could be parallelised that is missing an annotation.

\todo{Introduce these subsubsections}
\subsubsection{Infinite Thread Problem}
The sample programs shown in \autoref{sec:problem-details} assume that threads have no overhead and were used a a method of describing how a sequential program could be run in parallel. In the real world, these parallelised sample programs produce an unreasonable amount of threads of which most are waiting. This causes the parallel version to perform a lot worse than sequential code.

The initial solution for too many threads is to use a thread-pool so only a fixed number of threads are executed at the same time. This solution would not work in this case as if all the threads are waiting on a future task, which is also waiting for a thread to be free then we get stuck in a deadlock. By tweaking the design of the thread-pool slightly, we can have a fixed the number of tasks and prevent this deadlock.

As the task queue is the main cause of this deadlock, it is removed from this new proposed design which will be referred to as a no-queue thread-pool. Whenever a task is created, it looks for an available thread and that thread starts executing the task in the background. If there is no thread available, then the current thread should execute the task. Also, if the the current thread is waiting for a task to return, then it could execute another task whilst waiting. \todo{Display this if statement more visually?} \todo{Explain why this design won't deadlock like the traditional thread-pool}

To modify the sample programs to use the no-queue thread-pool would be of minimal work; it would require a shared memory space (to gain access to the threads) and instead of calling \texttt{thread::spawn}, it would call another function.

\subsubsection{Performance Analysis}

% TODO: Equalising
%\newpage
%\enlargethispage{-100mm}
\printbibliography
\end{document}
