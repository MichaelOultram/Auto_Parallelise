Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Feautrier1992,
abstract = {Programs and systems of recurrence equations may be represented as sets of actions which are to be executed subject to precedence constraints. In may cases, actions may be labelled by integral vectors in some iterations domains, and precedence constraints may be described by affine relations. A schedule for such a program is a function which assigns an execution date to each action. Knowledge of such a schedule allows one to estimate the intrinsic degree of parallelism of the program and to compile a parallel version for multiprocessor architectures or systolic arrays. This paper deals with the problem of finding closed form schedules as affine or piecewise affine functions of the iteration vector. An algorithm is presented which reduces the scheduling problem to a parametric linear program of small size, which can be readily solved by an efficient algorithm.},
author = {Feautrier, Paul},
file = {:home/moult/Documents/reference-papers/Feautrier1992.pdf:pdf},
journal = {International Journal of Parallel Programming},
keywords = {Automatic parallelization,automatic systolic array design,scheduling},
month = {oct},
number = {5},
pages = {313--347},
title = {{Some efficient solutions to the affine scheduling problem. I. One-dimensional time}},
volume = {21},
year = {1992}
}
@article{Dagum1998,
abstract = {At its most elemental level, OpenMP is a set of compiler directives and callable runtime library routines that extend Fortran (and separately, C and C++ to express shared memory parallelism. It leaves the base language unspecified, and vendors can implement OpenMP in any Fortran compiler. Naturally, to support pointers and allocatables, Fortran 90 and Fortran 95 require the OpenMP implementation to include additional semantics over Fortran 77. OpenMP leverages many of the X3H5 concepts while extending them to support coarse grain parallelism. The standard also includes a callable runtime library with accompanying environment variables.},
author = {Dagum, Leonardo and Menon, Ramesh},
file = {:home/moult/Documents/reference-papers/openmp-cse.pdf:pdf},
journal = {IEEE computational science and engineering},
number = {1},
pages = {46--55},
publisher = {IEEE},
title = {{OpenMP: an industry standard API for shared-memory programming}},
volume = {5},
year = {1998}
}
@inproceedings{Artigas2000,
abstract = {From a software engineering perspective, the Java programming language provides an attractive platform for writing numerically intensive applications. A major drawback hampering its widespread adoption in this domain has been its poor performance on numerical codes. This paper describes a prototype Java compiler which demonstrates that it is possible to achieve performance levels approaching those of current state-of-the-art C, C++ and Fortran compilers on numerical codes. We describe a new transformation called alias versioning that takes advantage of the simplicity of pointers in Java. This transformation, combined with other techniques that we have developed, enables the compiler to perform high order loop transformations (for better data locality) and parallelization completely automatically. We believe that our compiler is the first to have such capabilities of optimizing numerical Java codes. We achieve, with Java, between 80 and 100{\%} of the performance of highly optimized Fortran code in a variety of benchmarks. Furthermore, the automatic parallelization achieves speedups of up to 3.8 on four processors. Combining this compiler technology with packages containing the features expected by programmers of numerical applications would enable Java to become a serious contender for implementing new numerical applications.},
author = {Artigas, Pedro V. and Gupta, Manish and Midkiff, Samuel P. and Moreira, Jose E.},
booktitle = {Parallel Processing Letters},
doi = {10.1142/S0129626400000160},
file = {:home/moult/Documents/reference-papers/Artigas2000.pdf:pdf},
isbn = {1581132700},
issn = {0129-6264},
keywords = {Automatic loop transformations,Codes (symbols),Java programming language,Parallel processing systems,Prog},
number = {02n03},
pages = {153--164},
title = {{Automatic Loop Transformations and Parallelization for Java}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0129626400000160},
volume = {10},
year = {2000}
}
@article{Prabhu2010,
abstract = {Execution order constraints imposed by dependences can serialize computation, preventing parallelization of code and algorithms. Speculating on the value(s) carried by dependences is one way to break such critical dependences. Value speculation has been used effectively at a low level, by compilers and hardware. In this paper, we focus on the use of speculation by programmers as an algorithmic paradigm to parallelize seemingly sequential code. We propose two new language constructs, speculative composition and speculative iteration. These constructs enable programmers to declaratively express speculative parallelism in programs: to indicate when and how to speculate, increasing the parallelism in the program, without concerning themselves with mundane implementation details. We present a core language with speculation constructs and mutable state and present a formal operational semantics for the language. We use the semantics to define the notion of a correct speculative execution as one that is equivalent to a non-speculative execution. In general, speculation requires a runtime mechanism to undo the effects of speculative computation in the case of mis predictions. We describe a set of conditions under which such rollback can be avoided. We present a static analysis that checks if a given program satisfies these conditions. This allows us to implement speculation efficiently, without the overhead required for rollbacks. We have implemented the speculation constructs as a C{\#} library, along with the static checker for safety. We present an empirical evaluation of the efficacy of this approach to parallelization.},
author = {Prabhu, Prakash and Ramalingam, Ganesan and Vaswani, Kapil},
doi = {10.1145/1809028.1806603},
file = {:home/moult/Documents/reference-papers/Prabhu2010.pdf:pdf},
isbn = {9781450300193},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
title = {{Safe programmable speculative parallelism}},
year = {2010}
}
@article{Rauchwerger1999,
abstract = {Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is reexecuted serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching: it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks, which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods},
author = {Rauchwerger, Lawrence and Padua, David A.},
doi = {10.1109/71.752782},
file = {:home/moult/Documents/reference-papers/Rauchwerger1999.pdf:pdf},
isbn = {0-89791-697-2},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Compilers,DOALL,Parallel processing,Privatization,Reduction,Run-time,Speculative},
title = {{The LRPD test: Speculative run-time parallelization of loops with privatization and reduction parallelization}},
year = {1999}
}
@inproceedings{Bondhugula2008,
abstract = {The polyhedral model provides powerful abstractions to optimize loop nests with regular accesses. Affine transformations in this model capture a complex sequence of execution-reordering loop transformations that can improve performance by parallelization as well as locality enhancement. Although a significant body of research has addressed affine scheduling and partitioning, the problem of automaticallyfinding good affine transforms forcommunication-optimized coarsegrained parallelization together with locality optimization for the general case of arbitrarily-nested loop sequences remains a challenging problem. We propose an automatic transformation framework to optimize arbitrarilynested loop sequences with affine dependences for parallelism and locality simultaneously. The approach finds good tiling hyperplanes by embedding a powerful and versatile cost function into an Integer Linear Programming formulation. These tiling hyperplanes are used for communication-minimized coarse-grained parallelization as well as for locality optimization. The approach enables the minimization of inter-tile communication volume in the processor space, and minimization of reuse distances for local execution at each node. Programs requiring one-dimensional versusmulti-dimensional time schedules (with scheduling-based approaches) are all handled with the same algorithm. Synchronization-free parallelism, permutable loops or pipelined parallelismat various levels can be detected. Preliminary studies of the framework show promising results.},
author = {Bondhugula, Uday and Baskaran, Muthu and Krishnamoorthy, Sriram and Ramanujam, J. and Rountev, Atanas and Sadayappan, P.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-78791-4_9},
file = {:home/moult/Documents/reference-papers/Bondhugula2008.pdf:pdf},
isbn = {3540787909},
issn = {03029743},
title = {{Automatic transformations for communication-minimized parallelization and locality optimization in the polyhedral model}},
year = {2008}
}
@article{Verdoolaege2013,
abstract = {This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary.We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs.We evaluate our algorithms and tool on the entire PolyBench suite.},
author = {Verdoolaege, Sven and {Carlos Juega}, Juan and Cohen, Albert and {Ignacio G{\'{o}}mez}, Jos{\'{e}} and Tenllado, Christian and Catthoor, Francky},
doi = {10.1145/2400682.2400713},
file = {:home/moult/Documents/reference-papers/Verdoolaege2013.pdf:pdf},
issn = {15443566},
journal = {ACM Transactions on Architecture and Code Optimization},
title = {{Polyhedral parallel code generation for CUDA}},
year = {2013}
}
@inproceedings{Zhong2008,
abstract = {As multicore systems become the dominant mainstream computing technology, one of the most difficult challenges the industry faces is the software. Applications with large amounts of explicit thread-level parallelism naturally scale performance with the number of cores, but single-threaded applications realize little to no gains with additional cores. One solution to this problem is automatic parallelization that frees the programmer from the difficult task of parallel programming and offers hope for handling the vast amount of legacy single-threaded software. There is a long history of automatic parallelization for scientific applications, but the techniques have generally failed in the context of general-purpose software. Thread-level speculation overcomes the problem of memory dependence analysis by speculating unlikely dependences that serialize execution. However, this approach has lead to only modest performance gains. In this paper, we take another look at exploiting loop-level parallelism in single-threaded applications. We show that substantial amounts of loop-level parallelism is available in general-purpose applications, but it lurks beneath the surface and is often obfuscated by a small number of data and control dependences. We adapt and extend several code transformations from the instruction-level and scientific parallelization communities to uncover the hidden parallelism. Our results show that 61{\%} of the dynamic execution of studied benchmarks can be parallelized with our techniques compared to 27{\%} using traditional thread-level speculation techniques, resulting in a speedup of 1.84 on a four core system compared to 1.41 without transformations.},
author = {Zhong, Hongtao and Mehrara, Mojtaba and Lieberman, Steve and Mahlke, Scott},
booktitle = {Proceedings - International Symposium on High-Performance Computer Architecture},
doi = {10.1109/HPCA.2008.4658647},
file = {:home/moult/Documents/reference-papers/hongtaoz-hpca08.pdf:pdf},
isbn = {9781424420704},
issn = {15300897},
title = {{Uncovering hidden loop level parallelism in sequential applications}},
year = {2008}
}
@inproceedings{Baskaran2010,
abstract = {Graphics Processing Units (GPUs) offer tremendous computational power. CUDA (Compute Unified Device Architecture) provides a multi-threaded parallel programming model, facilitating high performance implementations of general-purpose computations. However, the explicitly managed memory hierarchy and multi-level parallel view make manual development of high-performance CUDA code rather complicated. Hence the automatic transformation of sequential input programs into efficient parallel CUDA programs is of considerable interest. This paper describes an automatic code transformation system that generates parallel CUDA code from input sequential C code, for regular (affine) programs. Using and adapting publicly available tools that have made polyhedral compiler optimization practically effective, we develop a C-to-CUDA transformation system that generates two-level parallel CUDA code that is optimized for efficient data access. The performance of automatically generated code is compared with manually optimized CUDA code for a number of benchmarks. The performance of the automatically generated CUDA code is quite close to hand-optimized CUDA code and considerably better than the benchmarks' performance on a multicore CPU.},
author = {Baskaran, Muthu and Ramanujam, Jj and Sadayappan, P},
booktitle = {Compiler Construction},
file = {:home/moult/Documents/reference-papers/Baskaran2010.pdf:pdf},
pages = {244--263},
title = {{Automatic C-to-CUDA code generation for affine programs}},
year = {2010}
}
@article{Eigenmann1998,
abstract = {This paper presents the results of the Cedar Hand-Parallelization Experiment conducted from 1989 through 1992, within the Center for Supercomputing Research and Development (CSRD) at the University of Illinois. In this experiment, we manually transformed the Perfect Benchmarks(R) into parallel program versions. In doing so, we used techniques that may be automated in an optimizing compiler. We then ran these programs on the Cedar multiprocessor (built at CSRD during the 1980s) and measured the speed improvement due to each technique. The results presented here extend the findings previously reported. The techniques credited most for the performance gains include array privatization, parallelization of reduction operations, and the substitution of generalized induction variables. All these techniques can be considered extensions of transformations that were available in vectorizers and commercial restructuring compilers of the late 1980s. We applied these transformations by hand to the given programs, in a mechanical manner, similar to that of a parallelizing compiler. Because of our success with these transformations, we believed that it would be possible to implement many of these techniques in a new parallelizing compiler. Such a compiler has been completed in the meantime and we show preliminary results.},
author = {Eigenmann, Rudolf and Hoeflinger, Jay and Padua, David},
file = {:home/moult/Documents/reference-papers/Eigenmann1998.pdf:pdf},
journal = {IEEE Transactions on Parallel and Distributed Systems},
number = {1},
pages = {5--23},
publisher = {IEEE},
title = {{On the automatic parallelization of the Perfect Benchmarks (R)}},
volume = {9},
year = {1998}
}
@article{Feautrier1992a,
abstract = {This paper extends the algorithms which were given in Part I to cases in which there is no aane schedule, i.e. to problems whose parallel com-plexity is polynomial but not linear. The natural generalization is to multi-dimensional schedules with lexicographic ordering as temporal succession. Multidimensional aane schedules, are, in a sense, equivalent to polyno-mial schedules, and are much easier to handle automatically. Furthermore, there is a strong connexion between multidimensional schedules and loop nests, which allows one to prove that a static control program always has a multidimensional schedule. Roughly, a larger dimension indicates less parallelism. In the algorithm which is presented here, this dimension is computed dynamically, and is just suucient for scheduling the source pro-gram. The algorithm lends itself to a $\backslash$divide and conquer" strategy. The paper gives some experimental evidence for the applicability, performances and limitations of the algorithm.},
author = {Feautrier, Paul},
file = {:home/moult/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feautrier - 1992 - Some eecient solutions to the aane scheduling problem Part II Multidimensional time.pdf:pdf},
title = {{Some efficient solutions to the afineane scheduling problem Part II Multidimensional time}},
url = {http://camlunity.ru/swap/Library/Conflux/Techniques - Code Analysis and Transformations (Polyhedral)/Lyon and Paris Universities (Feautrier, Bastoul)/Some Efficient Solutions to the Affine Scheduling Problem II.pdf},
year = {1992}
}
@inproceedings{Gasper2003,
abstract = {As the cost of hardware declines and the demand for computing power increases, it is becoming increasingly popular to turn to cluster computing. However, in order to gain the benefits of cluster computing, an existing software base must be converted to a parallel equivalent, or a new software base must be written. Both options require a developer skilled in both parallel programming, as well as the problem domain at hand. The ability to automate a conversion from sequential C code to a cluster-based equivalent offers a developer the power of parallel computing with a minimal learning curve. The following paper describes an ongoing project with the goal of automating the conversion from sequential C code to cluster-based parallel code. Currently the project relies on user input to guide the automation process, focusing on loop level parallelization. Long term goals center on using dependency analysis to automate the parallelization process.},
author = {Gasper, Pete and Herbst, Caleb and McCough, J and Rickett, Chris and Stubbendieck, Gregg},
booktitle = {Midwest Instruction and Computing Symposium, Duluth, MN, USA},
file = {:home/moult/Documents/reference-papers/Gasper2003.pdf:pdf},
title = {{Automatic parallelization of sequential C code}},
year = {2003}
}
@article{Geer2005,
abstract = {Computer performance has been driven largely by decreasing the size of chips while increasing the number of transistors they contain. In accordance with Moore's law, this has caused chip speeds to rise and prices to drop. This ongoing trend has driven much of the computing industry for years. Manufacturers are building chips with multiple cooler-running, more energy-efficient processing cores instead of one increasingly powerful core. The multicore chips don't necessarily run as fast as the highest performing single-core models, but they improve overall performance by handling more work in parallel. Multicores are a way to extend Moore's law so that the user gets more performance out of a piece of silicon. Chip makers AMD, IBM, Intel, and Sun are now introducing multicore chips for servers, desktops, and laptops.},
author = {Geer, David},
file = {:home/moult/Documents/reference-papers/Geer2005.pdf:pdf},
journal = {Computer},
number = {5},
pages = {11--13},
publisher = {IEEE},
title = {{Chip makers turn to multicore processors}},
volume = {38},
year = {2005}
}
@article{Feautrier1992b,
abstract = {This paper extends the algorithms which were given in Part I to cases in which there is no affine schedule, i.e. to problems whose parallel complexity is polynomial but not linear. The natural generalization is to multi-dimensional schedules with lexicographic ordering as temporal succession. Multidimensional affine schedules, are, in a sense, equivalent to polynomial schedules, and are much easier to handle automatically. Furthermore, there is a strong connexion between multidimensional schedules and loop nests, which allows one to prove that a static control program always has a multidimensional schedule. Roughly, a larger dimension indicates less parallelism. In the algorithm which is presented here, this dimension is computed dynamically, and is just sufficient for scheduling the source program. The algorithm lends itself to a "divide and conquer" strategy. The paper gives some experimental evidence for the applicability, performances and limitations of the algorithm.},
author = {Feautrier, Paul},
file = {:home/moult/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feautrier - 1992 - Some efficient solutions to the affine scheduling problem Part II Multidimensional time.pdf:pdf},
journal = {International Journal of Parallel Programming},
number = {6},
pages = {389--420},
title = {{Some efficient solutions to the affine scheduling problem Part II Multidimensional time}},
volume = {21},
year = {1992}
}
@inproceedings{Wang2009,
abstract = {The performance of single-threaded programs and legacy binary code is of critical importance in many everyday applications. However, neither can hardware multi-core processors directly speed up single-threaded programs, nor can software automatic parallelizing compilers effectively parallelize legacy binary code and irregular applications. In this paper, we propose a framework and a set of algorithms to dynamically parallelize single-threaded binary programs. Our parallelization is based on program slicing and explores both instruction-level parallelism (ILP) and thread-level parallelism (TLP). To significantly reduce the critical path of the parallel slices, our slicing algorithms exploit speculation to cut rare dependences, and use well-designed program transformations to expose parallelism. Furthermore, because we transparently parallelize binary code at runtime, we perform slicing only on program hot regions. Our experiments demonstrate that the proposed speculative slicing approach extracts more parallelism than any known slicing based parallelization schemes. For the SPEC2000 benchmarks, we can achieve 3x parallelism with infinite number of threads, and 1.8x parallelism with 4 threads.},
author = {Wang, Cheng and Wu, Youfeng and Borin, Edson and Hu, Shiliang and Liu, Wei and Sager, Dave and Ngai, Tin-fook and Fang, Jesse},
booktitle = {International Conference on Supercomputing (ICS)},
doi = {10.1145/1542275.1542302},
file = {:home/moult/Documents/reference-papers/ics018-wang.pdf:pdf},
isbn = {9781605584980},
keywords = {Backward Slicing,Binary Optimization,Parallelization,Single-Thread Performance,Speculations},
title = {{Dynamic Parallelization of Single-Threaded Binary Programs using Speculative Slicing}},
year = {2009}
}
@article{DHollander1998,
abstract = {The Fortran Parallel Transformer (FPT) is a parallelization tool for Fortran-77 programs. It is used for the automatic parallelization of loops, program transformations, dependence analysis, performance tuning and code generation for various platforms. FPT is able to deal with GOTO's by restructuring ill-structured code using hammock graph transformations. In this way more parallelism becomes detectable. The X-window based Programming Environment, PEFPT, extends FPT with interactive dependence analysis, the iteration space graph, ISG, and guided loop optimization. FPT contains a PVM (Parallel Virtual Machine) code generator which converts the parallel loops into PVM master- and slave-code for a network of workstations. This includes job scheduling, synchronization and optimized data communication. The productivity gained is about a factor of 10 in programming time and a significant speedup of the execution.},
author = {D'Hollander, Erik H and Zhang, Fubo and Wang, Qi},
file = {:home/moult/Documents/reference-papers/DHollander1998.pdf:pdf},
journal = {Information sciences},
number = {3-4},
pages = {293--317},
publisher = {Elsevier},
title = {{The fortran parallel transformer and its programming environment}},
volume = {106},
year = {1998}
}
@article{Kish2002,
abstract = {The exponential growth of memory size and clock frequency in computers has a great impact on everyday life. The growth is empirically described by Moore's law of miniaturization. Physical limitations of this growth would have a serious impact on technology and economy. A thermodynamical effect, the increasing thermal noise voltage (Johnson–Nyquist noise) on decreasing characteristic capacitances, together with the constrain of using lower supply voltages to keep power dissipation manageable on the contrary of increasing clock frequency, has the potential to break abruptly Moore's law within 6–8 years, or earlier.},
author = {Kish, Laszlo B},
file = {:home/moult/Documents/reference-papers/Kish2002.pdf:pdf},
journal = {Physics Letters A},
number = {3},
pages = {144--149},
publisher = {Elsevier},
title = {{End of Moore's law: thermal (noise) death of integration in micro and nano electronics}},
volume = {305},
year = {2002}
}
@misc{rustbook,
title = {{The Rust Book}},
url = {https://doc.rust-lang.org/book/},
urldate = {2017-11-12}
}
@article{Wang2008,
abstract = {The efficient mapping of program parallelism to multi-core processors is highly dependent on the underlying architecture. This paper proposes a portable and automatic compiler-based approach to mapping such parallelism using machine learning. It develops two predictors: a data sensitive and a data insensitive predictor to select the best mapping for parallel programs. They predict the number of threads and the scheduling policy for any given program using a model learnt off-line. By using low-cost profiling runs, they predict the mapping for a new unseen program across multiple input data sets. We evaluate our approach by selecting parallelism mapping configurations for OpenMP programs on two representative but different multi-core platforms (the Intel Xeon and the Cell processors). Performance of our technique is stable across programs and architectures. On average, it delivers above 96{\%} performance of the maximum available on both platforms. It achieve, on average, a 37{\%} (up to 17.5 times) performance improvement over the OpenMP runtime default scheme on the Cell platform. Compared to two recent prediction models, our predictors achieve better performance with a significant lower profiling cost.},
author = {Wang, Zheng and O'Boyle, Micheal F.P.},
doi = {10.1145/1504176.1504189},
file = {:home/moult/Documents/reference-papers/p75-wang.pdf:pdf},
isbn = {9781605583976},
issn = {03621340},
journal = {Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming - PPoPP '09},
number = {4},
title = {{Mapping parallelism to multi-cores: A Machine Learning Based Approach}},
volume = {44},
year = {2008}
}
@misc{rustlang,
title = {{The Rust Programming Language}},
url = {https://www.rust-lang.org},
urldate = {2017-11-12}
}
@book{Midkiff2012,
abstract = {Abstract Compiling for parallelism is a longstanding topic of compiler research. This book describes the fundamental principles of compiling "regular" numerical programs for parallelism. We begin with an explanation of analyses that allow a compiler to understand the interaction of data reads and writes in different statements and loop iterations during program execution. These analyses include dependence analysis, use-def analysis and pointer analysis. Next, we describe how the results of these analyses are used to enable transformations that make loops more amenable to parallelization, and discuss transformations that expose parallelism to target shared memory multicore and vector processors. We then discuss some problems that arise when parallelizing programs for execution on distributed memory machines. Finally, we conclude with an overview of solving Diophantine equations and suggestions for further readings in the topics of this book to enable the interested reader to delve deeper into the field. Ta...},
author = {Midkiff, Samuel P.},
booktitle = {Synthesis Lectures on Computer Architecture},
doi = {10.2200/S00340ED1V01Y201201CAC019},
isbn = {9781608458417},
issn = {1935-3235},
title = {{Automatic Parallelization: An Overview of Fundamental Compiler Techniques}},
volume = {7},
year = {2012}
}
@article{Quinones2005,
abstract = {Speculative parallelization can provide significant sources of additional thread-level parallelism, especially for irregular applications that are hard to parallelize by conventional approaches. In this paper, we present the Mitosis compiler, which partitions applications into speculative threads, with special emphasis on applications for which conventional parallelizing approaches fail. The management of inter-thread data dependences is crucial for the performance of the system. The Mitosis framework uses a pure software approach to predict/compute the thread's input values. This software approach is based on the use of pre-computation slices (p-slices), which are built by the Mitosis compiler and added at the beginning of the speculative thread. P-slices must compute thread input values accurately but they do not need to guarantee correctness, since the underlying architecture can detect and recover from misspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce their overhead. The most important optimizations included in the Mitosis compiler and presented in this paper are branch pruning, memory and register dependence speculation, and early thread squashing. Performance evaluation of Mitosis compiler/architecture shows an average speedup of 2.2. Copyright 2005 ACM.},
author = {Qui{\~{n}}ones, Carlos Garc{\'{i}}a and Madriles, Carlos and S{\'{a}}nchez, Jes{\'{u}}s and Marcuello, Pedro and Gonz{\'{a}}lez, Antonio and Tullsen, Dean M},
doi = {http://doi.acm.org/10.1145/1064978.1065043},
file = {:home/moult/Documents/reference-papers/Quinones2005.pdf:pdf},
isbn = {1595930566},
issn = {0362-1340},
journal = {ACM Sigplan Notices},
keywords = {all part,automatic parallelization,classroom use granted,copies not made,hard copies,permission make digital,pre computation slices,speculative multithreading,thread level parallelism,without fee provided,work personal},
title = {{Mitosis compiler: an infrastructure for speculative threading based on pre-computation slices}},
year = {2005}
}
@inproceedings{Beletska2011,
abstract = {Automatic coarse-grained parallelization of program loops is of great importance for parallel computing systems. This paper presents the theory of Iteration Space Slicing aimed at extracting synchronization-free parallelism available in arbitrarily nested program loops. We demonstrate that Iteration Space Slicing algorithms permits for extracting more coarse-grained parallelism than that extracted by means of the Affine Transformation Framework provided that we are able to calculate the transitive closure of the union of relations describing all dependences in the affine loop. Experimental results show that by means of Iteration Space Slicing algorithms, we are able to extract coarse-grained parallelism for many loops of NAS and UTDSP benchmarks. Problems to be resolved in order to enhance the theory of Iteration Space Slicing are discussed. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Beletska, Anna and Bielecki, Wlodzimierz and Cohen, Albert and Palkowski, Marek and Siedlecki, Krzysztof},
booktitle = {Parallel Computing},
doi = {10.1016/j.parco.2010.12.005},
file = {:home/moult/Documents/reference-papers/Beletska2011.pdf:pdf},
isbn = {9780769536804},
issn = {01678191},
keywords = {Affine transformations,Coarse-grained parallelism,Iteration Space Slicing},
title = {{Coarse-grained loop parallelization: Iteration Space Slicing vs affine transformations}},
year = {2011}
}
@book{Banerjee1993,
address = {Boston},
author = {Banerjee, Utpal},
isbn = {079239318X},
publisher = {Kluwer Academic Publishers},
title = {{Loop transformations for restructuring compilers: the foundations}},
year = {1993}
}
@article{Lam2011,
abstract = {Single core designs and architectures have reached their limits due to heat and power walls. In order to continue to increase hardware performance, hardware industries have moved forward to multi-core designs and implementations which introduces a new paradigm in parallel computing. As a result, software programmers must be able to explicitly write or produce parallel programs to fully exploit the potential computing power of parallel processing in the underlying multi-core architectures. Since the hardware solution directly exposes parallelism to software designers, different approaches have been investigated to help the programmers to implement software parallelism at different levels. One of the approaches is to dynamically parallelize serial programs at the binary level. Another approach is to use automatic parallelizing compilers. Yet another common approach is to manually insert parallel directives into serial codes to achieve parallelism. This writing project presents a machine learning and compiler-based approach to design and implement a system to automatically parallelize serial C programs via OpenMP directives. The system is able to learn and analyze source code parallelization mechanisms from a training set containing pre-parallelized programs with OpenMP constructs. It then automatically applies the knowledge learned onto serial programs to achieve parallelism. This automatic parallelizing approach can be used to target certain common parallel constructs or directives, and its results when combined with a manual parallelizing technique can achieve maximum or better parallelism in complex serial programs. Furthermore, the approach can also be used as part of compiler design to help improve both the speed and performance of a parallel compiler.},
author = {Lam, Nam Quang},
file = {:home/moult/Documents/reference-papers/Lam2011.pdf:pdf},
title = {{A Machine Learning and Compiler-based Approach to Automatically Parallelize Serial Programs Using OpenMP}},
year = {2011}
}
@article{Feng2012,
abstract = {Software-based thread-level parallelization has been widely studied for exploiting data parallelism in purely computational loops to improve program performance on multiprocessors. However, none of the previous efforts deal with efficient parallelization of hybrid loops, i.e., loops that contain a mix of computation and I/O operations. In this paper, we propose a set of techniques for efficiently parallelizing hybrid loops. Our techniques apply DOALL parallelism to hybrid loops by breaking the cross-iteration dependences caused by I/O operations. We also support speculative execution of I/O operations to enable speculative parallelization of hybrid loops. Helper threading is used to reduce the I/O bus contention caused by the improved parallelism. We provide an easy-to-use programming model for exploiting parallelism in loops with I/O operations. Parallelizing hybrid loops using our model requires few modifications to the code. We have developed a prototype implementation of our programming model. We have evaluated our implementation on a 24-core machine using eight applications, including a widely-used genomic sequence assembler and a multi-player game server, and others from PARSEC and SPEC CPU2000 benchmark suites. The hybrid loops in these applications take 23{\%}-99{\%} of the total execution time on our 24-core machine. The parallelized applications achieve speedups of 3.0x-12.8x with hybrid loop parallelization over the sequential versions of the same applications. Compared to the versions of applications where only computation loops are parallelized, hybrid loop parallelization improves the application performance by 68{\%} on average.},
author = {Feng, Min and Gupta, Rajiv and Neamtiu, Iulian},
doi = {10.1145/2345156.2254122},
file = {:home/moult/Documents/reference-papers/pldi12feng.pdf:pdf},
isbn = {9781450312059},
issn = {0362-1340},
journal = {PLDI: Programming Languages Design and Implementation},
number = {6},
title = {{Effective parallelization of loops in the presence of I/O operations}},
volume = {47},
year = {2012}
}
@inproceedings{Kim2000,
abstract = {We introduce ICU-PFC: an automatic parallelizing compiler. It receives FORTRAN source code and generates parallel FORTRAN code where OpenMP directives for parallel execution are inserted. A research compiler is developed to test automatic parallelizing techniques in the SMP environment. ICU-PFC detects DO ALL parallel loops and inserts appropriate OpenMP directives. For parallel loop detection, we designed and implemented a dependence matrix which is used for storing data dependence information of statements in a loop. In experimental results ICCT-PFC generated code showed better performance than sequential code and even manually parallelized code.},
author = {Kim, Hong Soog and Yoon, Young Ha and Na, Sang Og and Han, Dong Soo},
booktitle = {Proceedings - 4th International Conference/Exhibition on High Performance Computing in the Asia-Pacific Region, HPC-Asia 2000},
doi = {10.1109/HPC.2000.846552},
file = {:home/moult/Documents/reference-papers/Kim2000.pdf:pdf},
isbn = {0769505902},
title = {{ICU-PFC: An automatic parallelizing compiler}},
year = {2000}
}
@article{Yiapanis2015,
abstract = {Current parallelizing compilers can tackle applications exercising regular access patterns on arrays or affine indices, where data dependencies can be expressed in a linear form. Unfortunately, there are cases that independence between statements of code cannot be guaranteed and thus the compiler conservatively produces sequential code. Programs that involve extensive pointer use, irregular access patterns, and loops with unknown number of iterations are examples of such cases. This limits the extraction of parallelism in cases where dependencies are rarely or never triggered at runtime. Speculative parallelism refers to methods employed during program execution that aim to produce a valid parallel execution schedule for programs immune to static parallelization. The motivation for this article is to review recent developments in the area of compiler-driven software speculation for thread-level parallelism and how they came about. The article is divided into two parts. In the first part the fundamentals of speculative parallelization for thread-level parallelism are explained along with a design choice categorization for implementing such systems. Design choices include the ways speculative data is handled, how data dependence violations are detected and resolved, how the correct data are made visible to other threads, or how speculative threads are scheduled. The second part is structured around those design choices providing the advances and trends in the literature with reference to key developments in the area. Although the focus of the article is in software speculative parallelization, a section is dedicated for providing the interested reader with pointers and references for exploring similar topics such as hardware thread-level speculation, transactional memory, and automatic parallelization.},
author = {Yiapanis, Paraskevas and Brown, Gavin and Luj{\'{a}}n, Mikel},
doi = {10.1145/2821505},
file = {:home/moult/Documents/reference-papers/a5-yiapanis.pdf:pdf},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
keywords = {Thread-level speculation,automatic parallelization,multicore processors,runtime parallelization,speculative parallelization},
number = {2},
pages = {1--45},
title = {{Compiler-Driven Software Speculation for Thread-Level Parallelism}},
url = {http://dl.acm.org/citation.cfm?id=2866613.2821505},
volume = {38},
year = {2015}
}
@inproceedings{Canedo2010,
abstract = {The parallelization of Simulink applications is currently a responsibility of the system designer and the superscalar execution of the processors. State-of-the-art Simulink compilers excel at producing reliable and production-quality embedded code, but fail to exploit the natural concurrency available in the programs and to effectively use modern multi-core architectures. The reason may be that many Simulink applications are replete with loop-carried dependencies that inhibit most parallel computing techniques and compiler transformations. In this paper, we introduce the concept of strands that allow the data dependencies to be broken while preserving the original semantics of the Simulink program. Our fully automatic compiler transformations create a concurrent representation of the program, and thread-level parallelism for multi-core systems is planned and orchestrated. To improve single processor performance, we also exploit fine grain (equation-level) parallelism by level-order scheduling inside each thread. Our strand transformation has been implemented as an automatic transformation in a proprietary compiler and with a realistic aeronautic model executed in two processors leads to an up to 1.98 times speedup over uniprocessor execution, while the existing manual parallelization method achieves a 1.75 times speedup. {\textcopyright} 2010 ACM.},
author = {Canedo, A and Yoshizawa, T and Komatsu, H},
booktitle = {Proceedings of the 2010 CGO - The 8th International Symposium on Code Generation and Optimization},
doi = {10.1145/1772954.1772976},
file = {:home/moult/Documents/reference-papers/p151-canedo.pdf:pdf},
isbn = {9781605586359},
title = {{Automatic parallelization of simulink applications}},
year = {2010}
}
