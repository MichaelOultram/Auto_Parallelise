\chapter{Implementation}
\label{chapter:Implementation}
%\todo{Justify all decisions. Explain alternatives considered/implemented and why the design changed}

This chapter looks at how the design was implemented in practice and the design decisions that had to be adapted due to unforeseen complexities. Each design change is justified with an example of why the original design fails, and alternatives that were considered.

\section{How to Implement the Design}
There was three choices on how I could implement the design: directly modifying the rust compiler source code and recompiling the compiler; using the rust compiler plugin system to modify the abstract syntax tree (AST) or writing a source to source translation from scratch. Modifying the rust compiler would give me the flexibility to change any part of the compiler that I needed but it would make seeing my individual contributions very difficult. Also, the compiler itself is very large and complex; it would take a while to compile from a clean state. Using a rust compiler plugin would give me less access, but I would only need to compile my plugin. This option still has the downside of dealing with the complex compiler. Writing a source to source translation system from scratch would allow me to avoid touching the rust compiler and it's complexity. In return, I would have to write code to extract the AST from a source file. I would have to model the ownership/borrowing information to detect when parallelisms properly. From all these choices, I decided to write a rust compiler plugin as it provides all the ownership/borrowing information. This option requires me to use the full AST; the other options had the possibility of using the less verbose high-level intermediate representation (HIR).

The rust compiler allows for plugins of different types. The two types of plugins of interest are Syntax Extension plugins and Linter plugins. Syntax Extension plugins are executed first in the rust compiler pipeline, and are generally used to convert macros into code. Linter plugins are run at a later stage, and are generally used to check code style to produce warnings (like unused variable). It has the complete AST of the code with all the macros expanded. All the information required about dependencies is accessible inside a linter plugin. However, once the rust compiler gets to the linter plugins, the AST can no longer be modified. Syntax extension plugins allow modification of the AST, but they do not have macros expanded. Some dependencies could be missed by trying to analyse the AST at this stage. The solution I decided on was to compile the program twice. On the first compile, the syntax extension does nothing and the linter plugin examines the expanded code. The dependency information gathered is saved into a file for the next compile. On the second compile, the syntax extension plugin reads the file to get all the dependency information. Any parallelisable parts are then modified to be run in parallel.

To use the rust plugin system, each function of the sequential source code should be annotated with \texttt{\#[autoparallelise]}. This allows the plugins with access to that element in the AST. The compiler plugin also needs to be added as a dependency in the \texttt{Cargo.toml} file and the main file needs \texttt{\#![feature(plugin)]} and \texttt{\#![plugin(auto\_parallelise)]} at the top. These annotations do not provide any information about the parallelisability of the source code and is purely just a workaround for the rust plugin system. This is further explained in \autoref{sec:running-the-code}.

\section{Linter Plugin}
% Analysis Stage
%==================
% Convert from Rust AST to our representation of Blocks and Statements
% Examine each statement for variables
% Find out the previous statement that the variable is referenced.
% This is a dependency and recorded as a relative id for the block
% Inside a block, a new DependencyTree is created
% The function name, DependencyTree and some other meta data is stored into an list.
% This is repeated for all the functions which have the annotation

% Once all the function have been analysed.
% Each dependency tree is converted into an EncodedDependencyTree by taking the StmtID of each statement.
% StmtID is the span.lo().0 and span.hi().0 which refers to the byte in the original source code.

% All the EncodedDependencyTree's and function meta data is saved into a JSON file
In the design section of the report, the dependency analysis algorithm takes in a block and examines it statement by statement to extract out the variables that are used. Actually doing this in the compiler plugin required a lot more effort than it seems on the face of it. The compiler calls a function in the plugin for each annotated function. To get the \rustc{Block} struct out of this requires expanding the \rustc{FnKind} enum. A \rustc{Block} contains a list of statements (\rustc{Stmt}). The \rustc{Stmt} struct contains a \rustc{StmtKind} enum. This describes what kind of statement it is, either a local variable binding, an unexpanded macro, or an expression ending with or without a semicolon. For the variable binding, a \rustc{Local} struct is given which contains a \rustc{Path} representing the variable name. Some variables will be assigned a value at creation and this is represented as an \rustc{Expr} struct. Unexpanded macros contain some other types which represent the arguments to the macro, but I did not end up going any deeper into this. Statements which are expression with or without a semicolon will give an \rustc{Expr} struct. Similar to the \rustc{Stmt} struct, the \rustc{Expr} struct contains a \rustc{ExprKind} enum which represents the $39$ different types of expressions. Most of these options contain another \rustc{Expr}, but some contain other types like a \rustc{Pat} which is the second way a variable can be represented. Almost all of the different cases in \rustc{ExprKind} had to be dealt with to fully explore the tree and extract all the variables a statement uses (its environment).

Each statement is converted into our representation of the AST so that information can be stored about the dependencies. The original design used two types, a expr to represent a single statement and a block to represent a list of exprs and blocks. This design does not deal a \rustc{ExprKind} which contains a \rustc{Block} properly (i.e. a for loop). My first idea was to split the \rustc{Stmt} into a expr and a block and set the block to depend on the expr. This seemed to work for the dependency analysis stage, but not for the reconstruction stage. It was difficult to distinguish which block is the contents of the \rustc{ExprKind} if an external block was also dependent on the expr. I created a third type called exprblock which contains one expr and a list of blocks (as an if-else expression will require more than one block). Once all the statements for a function are converted into our representation, and each statement had an environment, then the environments could be matched up as described in Algorithm \ref{alg:dependency-analysis}. The dependencies ids are stored as array indexes relative to the block.

%Originally only the relative dependency ids were going to be added to our representation of the AST. Once I got to developing the scheduler stage, I realised that I needed the environment information for synclines. Without the environment it was difficult for the reconstrutor to know which variables should be sent down the syncline. By storing the environment of each statement, the scheduling algorithm can look at both the releasing and requiring statements to work out what variables should be sent.

I realised that just using one environment had a flaw which adds an extra unneeded dependency. If there are two let statement which have the same variable name, the second let statement will be dependent on the first which is not needed. I fixed this by using two environments, one for the variables that the statement depends on, and another environment which lists the variables that the statement releases/produces. As described in \autoref{sec:rust-language-features}, a variable's ownership can be moved, and this can be represented by adding it to the requirements environments and not including it in the releases environment. In practice, I used the two environments, but I did not check whether a variable gets moved or not. If the source code provided to my parallelising plugin is correct sequential code, then the variable will not be mentioned in a statement after the point it has been moved.

\section{Syntax Extension Plugin}
% Modification Stage
%==================
% First part of analysis stage is repeated for the modification stage.
% Modification stage does not have marcos expanded, and so some dependencies would be missed.
% The EncodedDependencyTree is loaded from the JSON file which should include the dependency of the expanded macros.
% The EncodedDependencyTree is merged with the DependencyTree from the Modification stage so that unexpanded macros have dependencies.
The original design did not require two separate compiles. To get the dependency information from the Linter plugin into the Syntax Extension plugin, it was saved to a JSON file using the serde\_json library. I could not store the raw AST with my added dependency information as the both the JSON library and the rust compiler are separate crates which I cannot edit. To combat this, I converted the dependency tree into an encoded dependency tree. Each \rustc{Stmt} and \rustc{Block} would be represented by a number (a statement id). These structs already have a \rustc{NodeId} element which is what I tried first. Unfortunately, this id was not consistent between compiles. Both of these structs also had a \rustc{Span} element which represents the bytes in the source code that the \rustc{Stmt} or \rustc{Block} represents. Since the source code would not be changed between compiles, this should remain consistent. My statement id became two numbers, extracted from the AST by accessing \texttt{span.lo().0} and \texttt{span.hi().0}.

When the Syntax Extension plugin detects the JSON file, it loads the encoded dependency information. There is no easy way to decode the dependency information, so I ran the dependency analysis algorithm again on the AST that the Syntax Extension plugin has access to. This time, macros will not be expanded, and so I had to add a new Mac type to represent this. I did not try to examine the \rustc{Macro} as I had the encoded dependency information. I copied the encoded dependency information into the dependency tree with unexpanded macros by matching up the statement ids. Later I realised that I should be replacing the dependency information with the encoded dependency information instead of trying to merge it. The reason for this is because the dependency tree with unexpanded macros would have no environment to start of with. When a statement past the macro tries to match up its environment, it will skip checking the macro and match with something above. When the dependencies were merged, the statement would end up depending on two different statements for the same variable (the macro from the expanded dependency tree and then the statement above).

The scheduling algorithm from the design section did not require many changes at all. The only real addition I had to add was an environment to the syncline. This allows the reconstructor to know which variables it should send down the channel, and what variables will be received in the new thread.
To do this our representation of the AST needs to be adjusted to keep store the environment along with the dependency ids for each statement. By storing the environment of each statement, the scheduling algorithm can look at both the releasing and requiring statements to work out what variables should be sent.

Once a schedule has being created, it needs to be turned back into code. First I gathered all of the synclines that are in the schedule, and I created a channel for each of them. The channel name was based off of the statement ids of the two points being synced, and the variable names that will be sent down the channel. Each tree in the list of the schedule was fully explored before starting the next tree. They are all given a separate thread, except the last one which uses the current thread. The rust compiler has a few macros for creating new statements which works well for some static statements, and even some statements where a variable name is changed. But in some cases I had to unwrap the entire statement to change one part and then recreate it all the way up again. This was quite annoying as some of the functions or structs I wanted to use were private in the compiler so I had to delve deeper to see what I could make, and work from that. When I managed to create the types, and reconstructed all the code, the compiler would not accept my changes. All the error messages it gave me used the original source code, and were not very helpful. I got the compiler to print out the changes I made, as code. I copied this into a separate folder and it compiled just fine. Searching the issues on the rust compiler github, I came across \href{https://github.com/rust-lang/rust/issues/46489}{$\#46489$} which seemed to relate to my issue. Since that issue was not fixed by the time I had to demonstrate my project, I ended up writing a bash script to copy the modified code into a separate file and compile for a third time.

When examining the parallelised source code, I noticed that the incorrect value was returned from a function/block in some cases. Most of the time, when this occurred, the return type would be incorrect and so the parallel code would not compile. The reconstructor puts all the tree elements into separate threads except the last tree which is executed in the main thread. Before the function exits, it joins all of the threads that were created. If nothing more was done, then the return value would be whatever the second to last thread returns. I changed this by storing the return value of the last tree element into a variable and setting the return value of the function to that. Doing this method makes the parallelised source code a lot harder to read, but since it does not need to be human readable, I think it is ok. This works most of the time as the last element in the schedule is usually the correct return value, but not always. To make it work more consistently, I reordered the trees in descending order by the largest statement id value in the tree. The idea behind this is that the return value would be towards the end of the file. Ideally, the dependency analysis stage should be changed to find the return values. This would also remove the need to join all the threads at the end of the function as most of the threads would be guaranteed to be terminated by synclines.

\todo{PROOF READ UP TO HERE}

One source of speedup for a sequential program is loops. This type of parallelism is described is studied deeply in the literature (\autoref{sec:related-work}). It was added very late into my project, and so I put a few extra restrictions on it so that I could get it to work in time. These restrictions could be changed to make it work on more types of loops but I focused on for loops where the variable being iterated is just a number. The scheduler does not know the difference between the block of the if statement and the block of a for loop. When it comes the time to put the block back into the statement, the reconstrutor checks what kind of ExprKind it is. If it is a for loop with the restrictions met, it tries to parallelise it. Any external dependencies for the block are passed between iterations using a separate syncline for each variable for each iteration. The algorithm starts from the top of the block looking for each of the external dependencies and places a receive from the last iteration. Then it starts from the bottom looking for the variables and places a send to the next iteration. If one external variable is received at the start of the block and sent at the end of the block then the for loop will be executed sequentially. The block containing the code for an iteration is placed within a thread and the code to create the channels between iterations is placed above it. All of this is placed within the for loop. Above the for loop is where the initial channels are created, and after the for loop is where the external dependencies are sent to the first iteration and received from the last iteration. This approach works for some examples, but as shown in \autoref{sec:evaluation}, it has many flaws.

\begin{comment}
\begin{code}
\begin{minted}{rust}
let a; // Local without init
a = {
    let b = vec![1,2,3]; // Local with init
    println!("{:?}", b); // Mac
    b.len() // Expr
}; // Semi
\end{minted}
\caption{Example showing different StmtKinds}
\end{code}
\end{comment}
