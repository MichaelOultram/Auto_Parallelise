\chapter{Discussion}
\label{sec:discussion}
\todo{PROOF READ UP TO HERE}
\section{Achievements}
As shown in the \hyperref[sec:evaluation]{evaluation section} of the report, my solution manages to separate inter-weaved programs pretty well. It tries to run as much as possible in parallel, and creates syncs for dependencies in different threads. If one statement directly depends on another, then it is placed in the same thread to try to reduce the number of unnecessary threads. Not all programs will compile as not all of the rust language features are implemented properly in the parallelising compiler. One great achievement that I am particularly proud of is the parallel for loops as it was very late into the project before I managed to get them to partially work. Getting the parallelising compiler to work on simple examples took longer than I originally thought, and for a few weeks I had no idea how I was going to parallelise for loops. Once I could parallelise for loops (even partially), I had an example that was not too specifically coded that ran faster parallelised.

\section{Improvements}
The for loop parallelisation could be vastly improved and extended to work with more types of loops. It is not known how many iteration a while loop will execute at compile time and so it is not known how many threads to spawn. The program could spawn one thread at a time and then check if it should terminate, but this would be slower than sequentially running the loop due to thread overhead. To parallelise this kind of loop, some speculative parallelism would have to take place to run faster. The program could spawn $20$ threads each running a separate iteration. Any external dependencies should be passed between iterations, as long as the termination condition is not met. If the termination condition is not met by the end of the $20$ threads, then it should spawn another batch. When the termination condition is finally met, all the other threads need to be cancelled. In the literature, speculative parallelism requires undoing the cancelled threads, but in this case the cancelled threads are not touching anything external, and so it is as if they have not been run.
%From what I can tell, there is not an easy way to cancel a thread in rust.

More features of the language could be dealt with to parallelise more program, more accurately. Unsafe blocks/functions are not dealt with properly in the current version. They allow for modification of a global variable as well as dereferencing a pure pointer value. Since it would be possible for the unsafe block/function to access a variable without using the variables name, and detecting this would be very difficult, the unsafe block/function should be dependent on all statements before it. That way, the program state in the parallel version should be exactly what the sequential program is. All statements after the unsafe block/function should also be dependent on the unsafe block/function to remain consistent and to stop another statement being scheduled at the same time as the unsafe block/function.

When a variable is sent down a channel, it is moved between threads. The type of the variable must implement the Send trait to allow this to happen. Most types allow for this, but not all types. The current compiler does not check if the type implements this, or even if the variable is owned and just assumes that it is fine. In the cases that a variable who's type does not implement the Send trait is chosen to be sent down the channel the parallel program will not compile.

The compiler does not check whether a variable is mutable or immutable and assumes all variables are mutable. If the compiler checked this, it may be able to find more parallelisms. Immutable variables are not going to change, and so they could be cloned (if the Clone trait is implemented) into each thread they are used. This would reduce the number of unnecessary syncs, increase the parallel performance. Cloning immutable variables would increase the memory footprint of the runtime parallel program, so some care should be taken to not copy extremely big variables, but in most cases computers have enough memory to cope.

It is important to manage the number of threads being used and the current compiler does not do this. As shown in the \hyperref[sec:evaluation]{evaluation section}, when too many threads are spawned, the program does not wait for more threads, it just panics. One technique to combat this is to combine threads together. Not all the threads spawned do enough stuff to overcome the overhead and so the parallel program runs slower. A performance analysis technique would need to be used to estimate whether it is faster of slower to run in a separate thread. If it would be slower, then the scheduler should combine threads to reduce the number of synclines. While this technique will help to combat too many threads, and would solve some of the overhead problems, it might not be enough in some large, computationally expensive parallelisable programs. A threadpool could be used to limit the number of threads that are being executed. Care must be taken to not cause a deadlock using this technique. If all the executing threads are waiting for a dependency in a future thread, then that future thread will not be run as the current threads will never finish.

One last improvement I would suggest to my program is to store the type information with the variable name when deconstructing. Rust has type inference which works most of the time, but as shown in the \hyperref[sec:evaluation]{evaluation section}, type inference sometimes fails. By including the type information with the variable, the reconstrutor could annotate each variable with a type in the reconstructed code.

\section{Approach Changes}
While the approach I took managed to find some parallelisms in the code, there are a few changes to the approach that might find more parallelisms more easily. These changes will not be easy to add into my implementation, and so would require starting (essentially) from scratch.

The first approach change I wish to suggest is moving away from statement level parallelisms and into expression level parallelisms. There may be one statement which adds two slow function calls together. If this is written in one statement, it will execute the two slow functions sequentially but expression level parallelisms could spot this. This change significantly increases the difficult of all the stages of the parallel compiler, and increases the chance that the program will return the wrong result.

I would also suggest using the high-level intermediate representation (HIR) instead of using the rust plugins that I used. Some of the code that my parallelising compiler cannot parallelise can be rewritten slightly to make it parallelisable. The main reason for this is there is many ways to write the same program. With a smaller number of expression types, parallelisms can be more focused as the parallelising compiler would not need to deal with as much syntax sugar.

Not all dependencies are directly visible as variables. In the \hyperref[sec:evaluation]{evaluation section}, it is shown that a common pattern for timing a function call is completely independent of the function call itself when looking at variables alone. The programmer intends for the timing circuit to actually time the function, but the parallel code created does not. Other external functions may also have a shared state (unsafe in rust, or an unsafe function call) and could produce other similar problems if the order is changed. The unsafe information may not be visible as if a safe function calls an unsafe function, the safe function remains safe. The parallel compiler would need access to all of the library function body to fully evaluate if it is safe to run in a different order. It may not be possible to detect if the functions can be run at a different time without hard coding.
